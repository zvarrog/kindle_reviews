"""–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ö–µ–º—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ parquet —Ñ–∞–π–ª–∞—Ö."""

import logging
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass
import pandas as pd

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
from .logging_config import get_logger

log = get_logger(__name__)


@dataclass
class DataValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""

    is_valid: bool
    errors: List[str]
    warnings: List[str]
    schema_info: Dict[str, Any]


@dataclass
class DataSchema:
    """–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö."""

    required_columns: Set[str]
    optional_columns: Set[str]
    column_types: Dict[str, str]  # –∫–æ–ª–æ–Ω–∫–∞ -> –æ–∂–∏–¥–∞–µ–º—ã–π —Ç–∏–ø
    numeric_ranges: Dict[str, tuple]  # —á–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ -> (min, max)
    text_constraints: Dict[str, Dict[str, Any]]  # —Ç–µ–∫—Å—Ç–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ -> –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è


# –°—Ö–µ–º–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ Kindle (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
KINDLE_REVIEWS_SCHEMA = DataSchema(
    required_columns={"reviewText", "overall"},
    optional_columns={
        "text_len",
        "word_count",
        "kindle_freq",
        "sentiment_score",
        "has_punctuation",
        "avg_word_length",
        "upper_ratio",
        "digit_ratio",
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Ç Spark –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        "reviewTime",
        "unixReviewTime",
        "asin",
        "reviewerID",
        "sentiment",
        "words",
        "user_review_count",
        "item_review_count",
        "user_avg_len",
        "item_avg_len",
        "rawFeatures",
        "tfidfFeatures",  # Spark vector –∫–æ–ª–æ–Ω–∫–∏
    },
    column_types={
        "reviewText": "object",
        "overall": ["int64", "int32"],  # –î–æ–ø—É—Å–∫–∞–µ–º –æ–±–∞ —Ç–∏–ø–∞
        "text_len": ["float64", "int32", "int64"],  # –ì–∏–±–∫–∏–µ —Ç–∏–ø—ã
        "word_count": ["float64", "int32", "int64"],
        "kindle_freq": ["float64", "int32"],
        "sentiment_score": ["float64", "int32"],
        "has_punctuation": ["float64", "int32"],
        "avg_word_length": ["float64", "int32"],
        "upper_ratio": ["float64", "int32"],
        "digit_ratio": ["float64", "int32"],
    },
    numeric_ranges={
        "overall": (1, 5),
        "text_len": (0, 50000),
        "word_count": (0, 10000),
        "kindle_freq": (0.0, 50.0),  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∞–∫—Å–∏–º—É–º
        "sentiment_score": (-1.0, 1.0),
        "has_punctuation": (0.0, 1.0),
        "avg_word_length": (0.0, 50.0),
        "upper_ratio": (0.0, 1.0),
        "digit_ratio": (0.0, 1.0),
    },
    text_constraints={
        "reviewText": {"min_length": 1, "max_length": 100000, "allow_null": False}
    },
)


def validate_column_schema(
    df: pd.DataFrame, schema: DataSchema
) -> tuple[List[str], List[str]]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã –∫–æ–ª–æ–Ω–æ–∫."""
    errors = []
    warnings = []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    missing_required = schema.required_columns - set(df.columns)
    if missing_required:
        errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_required}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
    for col, expected_types in schema.column_types.items():
        if col in df.columns:
            actual_type = str(df[col].dtype)
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —Ç–∏–ø, —Ç–∞–∫ –∏ —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤
            if isinstance(expected_types, list):
                if actual_type not in expected_types:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': –æ–∂–∏–¥–∞–ª—Å—è –æ–¥–∏–Ω –∏–∑ —Ç–∏–ø–æ–≤ {expected_types}, –Ω–∞–π–¥–µ–Ω {actual_type}"
                    )
            else:
                if actual_type != expected_types:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': –æ–∂–∏–¥–∞–ª—Å—è —Ç–∏–ø {expected_types}, –Ω–∞–π–¥–µ–Ω {actual_type}"
                    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    all_expected = schema.required_columns | schema.optional_columns
    unexpected = set(df.columns) - all_expected
    if unexpected:
        warnings.append(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {unexpected}")

    return errors, warnings


def validate_data_quality(
    df: pd.DataFrame, schema: DataSchema
) -> tuple[List[str], List[str]]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö."""
    errors = []
    warnings = []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    if len(df) == 0:
        errors.append("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç")
        return errors, warnings

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
    for col in schema.required_columns:
        if col in df.columns:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                errors.append(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {null_count} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    for col, (min_val, max_val) in schema.numeric_ranges.items():
        if col in df.columns:
            numeric_data = pd.to_numeric(df[col], errors="coerce")
            if numeric_data.isnull().any():
                non_numeric_count = numeric_data.isnull().sum() - df[col].isnull().sum()
                if non_numeric_count > 0:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {non_numeric_count} –Ω–µ-—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"
                    )

            valid_data = numeric_data.dropna()
            if len(valid_data) > 0:
                below_min = (valid_data < min_val).sum()
                above_max = (valid_data > max_val).sum()
                if below_min > 0:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {below_min} –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∏–∂–µ –º–∏–Ω–∏–º—É–º–∞ {min_val}"
                    )
                if above_max > 0:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {above_max} –∑–Ω–∞—á–µ–Ω–∏–π –≤—ã—à–µ –º–∞–∫—Å–∏–º—É–º–∞ {max_val}"
                    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    for col, constraints in schema.text_constraints.items():
        if col in df.columns:
            text_data = df[col].astype(str)

            if not constraints.get("allow_null", True):
                null_or_empty = (
                    df[col].isnull() | (text_data == "") | (text_data == "nan")
                ).sum()
                if null_or_empty > 0:
                    errors.append(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {null_or_empty} –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

            min_len = constraints.get("min_length", 0)
            max_len = constraints.get("max_length", float("inf"))

            valid_texts = text_data[~df[col].isnull()]
            if len(valid_texts) > 0:
                lengths = valid_texts.str.len()
                too_short = (lengths < min_len).sum()
                too_long = (lengths > max_len).sum()

                if too_short > 0:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {too_short} —Ç–µ–∫—Å—Ç–æ–≤ –∫–æ—Ä–æ—á–µ {min_len} —Å–∏–º–≤–æ–ª–æ–≤"
                    )
                if too_long > 0:
                    warnings.append(
                        f"–ö–æ–ª–æ–Ω–∫–∞ '{col}': {too_long} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ {max_len} —Å–∏–º–≤–æ–ª–æ–≤"
                    )

    return errors, warnings


def validate_data_consistency(
    dataframes: Dict[str, pd.DataFrame],
) -> tuple[List[str], List[str]]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö."""
    errors = []
    warnings = []

    if len(dataframes) < 2:
        return errors, warnings

    # –ü–æ–ª—É—á–∞–µ–º —Å—Ö–µ–º—ã –≤—Å–µ—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
    schemas = {name: set(df.columns) for name, df in dataframes.items()}

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ö–µ–º—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
    reference_schema = next(iter(schemas.values()))
    reference_name = next(iter(schemas.keys()))

    for name, schema in schemas.items():
        if name == reference_name:
            continue

        missing_cols = reference_schema - schema
        extra_cols = schema - reference_schema

        if missing_cols:
            warnings.append(
                f"–í '{name}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ '{reference_name}': {missing_cols}"
            )
        if extra_cols:
            warnings.append(f"–í '{name}' –µ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {extra_cols}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    if all("overall" in df.columns for df in dataframes.values()):
        distributions = {}
        for name, df in dataframes.items():
            dist = df["overall"].value_counts(normalize=True).sort_index()
            distributions[name] = dist

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        reference_dist = distributions[reference_name]
        for name, dist in distributions.items():
            if name == reference_name:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è (>10% —Ä–∞–∑–Ω–∏—Ü—ã)
            for rating in reference_dist.index:
                if rating in dist.index:
                    diff = abs(reference_dist[rating] - dist[rating])
                    if diff > 0.1:
                        warnings.append(
                            f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ {rating}: "
                            f"{reference_name}={reference_dist[rating]:.3f}, "
                            f"{name}={dist[rating]:.3f} (—Ä–∞–∑–Ω–∏—Ü–∞ {diff:.3f})"
                        )

    return errors, warnings


def validate_parquet_file(
    file_path: Path,
    schema: DataSchema = KINDLE_REVIEWS_SCHEMA,
    check_duplicates: bool = True,
) -> DataValidationResult:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ parquet —Ñ–∞–π–ª–∞."""
    errors = []
    warnings = []
    schema_info = {}

    try:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        log.info("–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞: %s", file_path)
        df = pd.read_parquet(file_path)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º dtypes (–∏–∑–±–µ–≥–∞–µ–º numpy arrays)
        safe_dtypes = {}
        for col, dtype in df.dtypes.items():
            try:
                safe_dtypes[col] = str(dtype)
            except Exception:
                safe_dtypes[col] = "complex_type"

        schema_info.update(
            {
                "file_path": str(file_path),
                "rows": len(df),
                "columns": list(df.columns),
                "dtypes": safe_dtypes,
                "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024,
            }
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã
        schema_errors, schema_warnings = validate_column_schema(df, schema)
        errors.extend(schema_errors)
        warnings.extend(schema_warnings)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        quality_errors, quality_warnings = validate_data_quality(df, schema)
        errors.extend(quality_errors)
        warnings.extend(quality_warnings)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if check_duplicates and len(df) > 0:
            try:
                duplicates = df.duplicated().sum()
                if duplicates > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫")
                    schema_info["duplicates"] = duplicates
            except Exception as e:
                warnings.append(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã: {e}")
                schema_info["duplicates"] = "error_checking"

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if "overall" in df.columns:
            try:
                schema_info["rating_distribution"] = (
                    df["overall"].value_counts().to_dict()
                )
            except Exception:
                schema_info["rating_distribution"] = "error_calculating"

        if "reviewText" in df.columns:
            try:
                text_stats = {
                    "avg_text_length": df["reviewText"].astype(str).str.len().mean(),
                    "min_text_length": df["reviewText"].astype(str).str.len().min(),
                    "max_text_length": df["reviewText"].astype(str).str.len().max(),
                }
                schema_info["text_stats"] = text_stats
            except Exception:
                schema_info["text_stats"] = "error_calculating"

    except Exception as e:
        errors.append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        schema_info["error"] = str(e)

    is_valid = len(errors) == 0

    return DataValidationResult(
        is_valid=is_valid, errors=errors, warnings=warnings, schema_info=schema_info
    )


def validate_parquet_dataset(
    data_dir: Path,
    schema: DataSchema = KINDLE_REVIEWS_SCHEMA,
    check_consistency: bool = True,
) -> Dict[str, DataValidationResult]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ parquet —Ñ–∞–π–ª–æ–≤ (train/val/test)."""
    log.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è dataset –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {data_dir}")

    results = {}
    dataframes = {}

    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for split_name in ["train", "val", "test"]:
        file_path = data_dir / f"{split_name}.parquet"

        if file_path.exists():
            result = validate_parquet_file(file_path, schema)
            results[split_name] = result

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            if result.is_valid and check_consistency:
                try:
                    dataframes[split_name] = pd.read_parquet(file_path)
                except Exception as e:
                    log.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {split_name} –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {e}"
                    )
        else:
            log.warning(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            results[split_name] = DataValidationResult(
                is_valid=False,
                errors=[f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"],
                warnings=[],
                schema_info={"file_path": str(file_path), "exists": False},
            )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
    if check_consistency and len(dataframes) > 1:
        consistency_errors, consistency_warnings = validate_data_consistency(dataframes)

        # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        consistency_result = DataValidationResult(
            is_valid=len(consistency_errors) == 0,
            errors=consistency_errors,
            warnings=consistency_warnings,
            schema_info={
                "type": "consistency_check",
                "files_compared": list(dataframes.keys()),
            },
        )
        results["consistency"] = consistency_result

    return results


def log_validation_results(results: Dict[str, DataValidationResult]) -> bool:
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –≤—Å–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–æ—à–ª–∏."""
    all_valid = True

    for name, result in results.items():
        log.info(f"\nüìä –í–∞–ª–∏–¥–∞—Ü–∏—è '{name}':")
        log.info(f"  –°—Ç–∞—Ç—É—Å: {'‚úÖ –£—Å–ø–µ—à–Ω–æ' if result.is_valid else '‚ùå –û—à–∏–±–∫–∏'}")

        if result.errors:
            all_valid = False
            log.error(f"  –û—à–∏–±–∫–∏ ({len(result.errors)}):")
            for error in result.errors:
                log.error(f"    ‚Ä¢ {error}")

        if result.warnings:
            log.warning(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è ({len(result.warnings)}):")
            for warning in result.warnings:
                log.warning(f"    ‚Ä¢ {warning}")

        # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if "rows" in result.schema_info:
            log.info(f"  üìà –°—Ç—Ä–æ–∫: {result.schema_info['rows']:,}")
        if "columns" in result.schema_info:
            log.info(f"  üìã –ö–æ–ª–æ–Ω–æ–∫: {len(result.schema_info['columns'])}")
        if "memory_usage_mb" in result.schema_info:
            log.info(f"  üíæ –ü–∞–º—è—Ç—å: {result.schema_info['memory_usage_mb']:.1f} MB")

    return all_valid
