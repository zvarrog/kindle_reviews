"""–û–±—Ä–∞–±–æ—Ç–∫–∞ kindle_reviews.csv c –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏.

–ò–∑–º–µ–Ω–µ–Ω–∏—è/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
* –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ (leading comma / _c0)
* –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º (–∑–≤—ë–∑–¥–∞–º) —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –ª–∏–º–∏—Ç–æ–º –ø–æ –∫–ª–∞—Å—Å—É
* –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–∞ shuffle (–∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ç–æ–≤–∞—Ä–∞ –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ –∫–∞–∂–¥–∞—è)
* –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —á–∏—Å–ª–æ shuffle –ø–∞—Ä—Ç–∏—Ü–∏–π –¥–ª—è –º–∞–ª—ã—Ö/—Å—Ä–µ–¥–Ω–∏—Ö –æ–±—ä—ë–º–æ–≤
"""

from pathlib import Path
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    length,
    split,
    count,
    avg,
    row_number,
    size,
    lower,
    when,
    regexp_replace,
    substring,
)
from pyspark.sql.window import Window
from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF
from pyspark import StorageLevel
from scripts.settings import (
    FORCE_PROCESS,
    RAW_DATA_DIR,
    CSV_NAME,
    PROCESSED_DATA_DIR,
    PER_CLASS_LIMIT,
    HASHING_TF_FEATURES,
    SHUFFLE_PARTITIONS,
    MIN_DF,
    MIN_TF,
)


TRAIN_PATH = PROCESSED_DATA_DIR / "train.parquet"
VAL_PATH = PROCESSED_DATA_DIR / "val.parquet"
TEST_PATH = PROCESSED_DATA_DIR / "test.parquet"

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ä–µ–¥—ã)
from .logging_config import setup_auto_logging

log = setup_auto_logging()

if (
    not FORCE_PROCESS
    and TRAIN_PATH.exists()
    and VAL_PATH.exists()
    and TEST_PATH.exists()
):
    log.warning(
        "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ %s. –î–ª—è —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ñ–ª–∞–≥ FORCE_PROCESS = True.",
        str(PROCESSED_DATA_DIR),
    )
else:

    # –°–æ–∑–¥–∞—ë–º SparkSession (–µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Å–∞–º –ø–æ —Å–µ–±–µ)
    try:
        spark = (
            SparkSession.builder.appName("KindleReviews")
            .config("spark.driver.memory", "1g")
            .config("spark.executor.memory", "1g")
            .config("spark.sql.adaptive.enabled", "true")
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
            .config("spark.sql.execution.arrow.pyspark.enabled", "true")
            .getOrCreate()
        )
    except Exception:
        # –µ—Å–ª–∏ SparkSession —É–∂–µ —Å–æ–∑–¥–∞–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏, –ø—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∏–º —Ç–µ–∫—É—â–∏–π
        spark = SparkSession.builder.getOrCreate()

    # –°–Ω–∏–∂–∞–µ–º —á–∏—Å–ª–æ shuffle –ø–∞—Ä—Ç–∏—Ü–∏–π –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –æ–±—ä—ë–º–æ–≤
    try:
        spark.conf.set("spark.sql.shuffle.partitions", str(SHUFFLE_PARTITIONS))
    except Exception:
        pass

    log.info(
        "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: driverMemory=%s, executorMemory=%s, shuffle.partitions=%s",
        spark.sparkContext.getConf().get("spark.driver.memory"),
        spark.sparkContext.getConf().get("spark.executor.memory"),
        spark.conf.get("spark.sql.shuffle.partitions"),
    )

    df = spark.read.csv(
        str(RAW_DATA_DIR / CSV_NAME),
        header=True,
        inferSchema=True,
        quote='"',
        escape='"',
        multiLine=True,
    )

    # –£—Å—Ç–æ–π—á–∏–≤–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ (leading comma / _c0 / BOM)
    first_col = df.columns[0]
    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è: —É–±–∏—Ä–∞–µ–º BOM –∏ –ø—Ä–æ–±–µ–ª—ã
    if isinstance(first_col, str):
        cleaned = first_col.strip()
        if cleaned.startswith("\ufeff"):
            cleaned = cleaned.lstrip("\ufeff")
    else:
        cleaned = first_col

    expected_cols = [
        "asin",
        "helpful",
        "overall",
        "reviewText",
        "reviewTime",
        "reviewerID",
        "reviewerName",
        "summary",
        "unixReviewTime",
    ]

    should_drop = False
    # —è–≤–Ω–æ –ø—É—Å—Ç–æ–µ –∏–º—è –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–º—è –ø–∞—Ä—Å–µ—Ä–∞
    if cleaned in ("", "_c0"):
        should_drop = True
    # –µ—Å–ª–∏ –Ω–∞ –æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É –±–æ–ª—å—à–µ, —á–µ–º –æ–∂–∏–¥–∞–µ—Ç—Å—è, –∏ –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –Ω–µ –∏–∑ –æ–∂–∏–¥–∞–µ–º—ã—Ö
    elif len(df.columns) == len(expected_cols) + 1 and cleaned not in expected_cols:
        should_drop = True

    if should_drop:
        log.info("–£–¥–∞–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞: raw=%r cleaned=%r", first_col, cleaned)
        df = df.drop(first_col)
        log.info("–ù–æ–≤—ã–π header: %s", ", ".join(df.columns))
    else:
        log.info(
            "–ü–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤–∞–ª–∏–¥–Ω—ã–π (raw=%r cleaned=%r) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —É–¥–∞–ª–µ–Ω–∏–µ",
            first_col,
            cleaned,
        )

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    cols = [
        "reviewerID",
        "asin",
        "reviewText",
        "overall",
        "unixReviewTime",
        "reviewTime",
    ]
    df = df.select(*cols)
    # –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (truncate, lower, normalize symbols, remove html/url/non-latin, collapse spaces)
    MAX_TEXT_CHARS = 2000
    text_expr = lower(substring(col("reviewText"), 1, MAX_TEXT_CHARS))
    # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ –ø—Ä–æ–±–µ–ª—ã/–º–∞—Ä–∫–∏: zero-width space, BOM, NBSP
    text_expr = regexp_replace(text_expr, r"[\u200b\ufeff\u00A0]", " ")
    # –£–±–∏—Ä–∞–µ–º HTML/URL
    text_expr = regexp_replace(text_expr, r"<[^>]+>", " ")
    text_expr = regexp_replace(text_expr, r"http\S+", " ")
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏—Ö –∫–∞–≤—ã—á–µ–∫ –∏ —Ç–∏—Ä–µ
    text_expr = regexp_replace(text_expr, r"[\u2018\u2019]", "'")
    text_expr = regexp_replace(text_expr, r"[\u201C\u201D]", '"')
    text_expr = regexp_replace(text_expr, r"[\u2013\u2014]", "-")
    # –£–±–∏—Ä–∞–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ Kindle/ebook-–º–µ—Ç–∫–∏, –Ω–µ –Ω–µ—Å—É—â–∏–µ —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
    text_expr = regexp_replace(
        text_expr,
        r"\b(kindle edition|prime reading|whispersync|borrow(?:ed)? for free|free sample|look inside)\b",
        " ",
    )
    # –¢–æ–ª—å–∫–æ –ª–∞—Ç–∏–Ω–∏—Ü–∞ –∏ –ø—Ä–æ–±–µ–ª—ã, —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤
    text_expr = regexp_replace(text_expr, r"[^a-z ]", " ")
    text_expr = regexp_replace(text_expr, r"\s+", " ")
    df = df.withColumn("reviewText", text_expr)

    # –ß–∏—Å—Ç–∏–º –¥–∞–Ω–Ω—ã–µ: –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∏
    clean = df.filter((col("reviewText").isNotNull()) & (col("overall").isNotNull()))
    log.info("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ null: %s", clean.count())

    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞: –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ n –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É
    window = Window.partitionBy("overall").orderBy(col("unixReviewTime").desc())
    clean = (
        clean.withColumn("row_num", row_number().over(window))
        .filter(col("row_num") <= PER_CLASS_LIMIT)
        .drop("row_num")
    )
    balanced_count = clean.count()
    log.info(
        "–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (<= %d –Ω–∞ –∫–ª–∞—Å—Å) —Å—Ç—Ä–æ–∫: %d", PER_CLASS_LIMIT, balanced_count
    )

    clean = clean.withColumn("text_len", length(col("reviewText")))
    clean = clean.withColumn("word_count", size(split(col("reviewText"), " ")))
    # –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ 'kindle' –≤ –æ—Ç–∑—ã–≤–µ
    clean = clean.withColumn(
        "kindle_freq", size(split(lower(col("reviewText")), "kindle")) - 1
    )

    # Sentiment (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: +1 –µ—Å–ª–∏ –µ—Å—Ç—å 'good', 'excellent', -1 –µ—Å–ª–∏ 'bad', 'poor', 0 –∏–Ω–∞—á–µ)
    clean = clean.withColumn(
        "sentiment",
        when(lower(col("reviewText")).contains("good"), 1)
        .when(lower(col("reviewText")).contains("excellent"), 1)
        .when(lower(col("reviewText")).contains("bad"), -1)
        .when(lower(col("reviewText")).contains("poor"), -1)
        .otherwise(0),
    )

    # –î–µ–ª–∏–º –Ω–∞ –≤—ã–±–æ—Ä–∫–∏
    train, val, test = clean.randomSplit([0.7, 0.15, 0.15], seed=42)
    tr_c, v_c, te_c = train.count(), val.count(), test.count()
    log.info(
        "–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: train=%d, val=%d, test=%d (total=%d)",
        tr_c,
        v_c,
        te_c,
        tr_c + v_c + te_c,
    )

    # TF-IDF: —Ñ–∏—Ç–∏–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ IDF —Ç–æ–ª—å–∫–æ –Ω–∞ train –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –∫ val/test —Ç–µ–º –∂–µ –º–æ–¥–µ–ª—è–º
    tokenizer = Tokenizer(inputCol="reviewText", outputCol="words")
    vectorizer = CountVectorizer(
        inputCol="words",
        outputCol="rawFeatures",
        vocabSize=HASHING_TF_FEATURES,
        minDF=MIN_DF,
        minTF=MIN_TF,
    )
    idf = IDF(inputCol="rawFeatures", outputCol="tfidfFeatures")

    train_words = tokenizer.transform(train)
    vec_model = vectorizer.fit(train_words)
    train_feat = vec_model.transform(train_words)
    idfModel = idf.fit(train_feat)
    train = idfModel.transform(train_feat)

    val_words = tokenizer.transform(val)
    val_feat = vec_model.transform(val_words)
    val = idfModel.transform(val_feat)

    test_words = tokenizer.transform(test)
    test_feat = vec_model.transform(test_words)
    test = idfModel.transform(test_feat)

    train = train.persist(StorageLevel.MEMORY_AND_DISK)
    val = val.persist(StorageLevel.MEMORY_AND_DISK)
    test = test.persist(StorageLevel.MEMORY_AND_DISK)
    log.info(
        "CountVectorizer: vocabSize<=%d, minDF=%s, minTF=%s",
        HASHING_TF_FEATURES,
        str(MIN_DF),
        str(MIN_TF),
    )

    # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–∞ train
    user_stats = train.groupBy("reviewerID").agg(
        avg("text_len").alias("user_avg_len"),
        count("reviewText").alias("user_review_count"),
    )
    item_stats = train.groupBy("asin").agg(
        avg("text_len").alias("item_avg_len"),
        count("reviewText").alias("item_review_count"),
    )

    # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã –∫ –∫–∞–∂–¥–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
    train = train.join(user_stats, on="reviewerID", how="left").join(
        item_stats, on="asin", how="left"
    )
    val = val.join(user_stats, on="reviewerID", how="left").join(
        item_stats, on="asin", how="left"
    )
    test = test.join(user_stats, on="reviewerID", how="left").join(
        item_stats, on="asin", how="left"
    )

    log.info(
        "–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ –∫–æ–ª-–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –≤ train: %d", len(train.columns)
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    train.write.mode("overwrite").parquet(TRAIN_PATH)
    val.write.mode("overwrite").parquet(VAL_PATH)
    test.write.mode("overwrite").parquet(TEST_PATH)

    log.info(
        "–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ %s",
        str(Path(PROCESSED_DATA_DIR).resolve()),
    )

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    log.info("üîç –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö parquet —Ñ–∞–π–ª–æ–≤...")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pandas –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏ Spark
        from .data_validation import validate_parquet_dataset, log_validation_results

        validation_results = validate_parquet_dataset(Path(PROCESSED_DATA_DIR))
        all_valid = log_validation_results(validation_results)

        if all_valid:
            log.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        else:
            log.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    except Exception as e:
        log.warning(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")

    log.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    spark.stop()
